{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import sigmoid, tanh\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, Dropout, Embedding, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b685cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X, y = np.vstack((X_train, X_test)), np.hstack((y_train, y_test))\n",
    "X = (X  - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706dac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator(input_shape=(28,28,1), classes_number=10):\n",
    "    label_input = Input(shape=(1,))\n",
    "    \n",
    "    label_layer = Embedding(input_dim=classes_number, output_dim=50)(label_input)\n",
    "    label_layer = Dense(units=input_shape[0] * input_shape[1])(label_layer)\n",
    "    label_layer = Reshape(target_shape=(input_shape[0], input_shape[1], 1))(label_layer)\n",
    "    \n",
    "    image_input = Input(shape=input_shape)\n",
    "    \n",
    "    concatenated_input = Concatenate()([image_input, label_layer])\n",
    "    \n",
    "    layer = Conv2D(128, kernel_size=(3,3), strides=(2,2), padding='same')(concatenated_input)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "    \n",
    "    layer = Conv2D(filters=128, kernel_size=(3,3), strides=(2,2), padding='same')(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    layer = Dropout(rate=0.4)(layer)\n",
    "    output_layer = Dense(units=1, activation=sigmoid)(layer)\n",
    "    \n",
    "    \n",
    "    discriminator = Model([image_input, label_input], output_layer)\n",
    "    discriminator.compile(loss=BinaryCrossentropy(), optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "    \n",
    "    return discriminator\n",
    " \n",
    "\n",
    "def create_generator(hidden_dim, classes_number=10):\n",
    "    label_input = Input(shape=(1,))\n",
    "    \n",
    "    label_layer = Embedding(input_dim=classes_number, output_dim=50)(label_input)\n",
    "    label_layer = Dense(units=7 * 7)(label_layer)\n",
    "    label_layer = Reshape(target_shape=(7, 7, 1))(label_layer)\n",
    "    \n",
    "    noise_input = Input(shape=(hidden_dim,))\n",
    "                                       \n",
    "    noise_layer = Dense(units=7 * 7 * 128)(noise_input)\n",
    "    noise_layer = LeakyReLU(alpha=0.2)(noise_layer)\n",
    "    noise_layer = Reshape((7, 7, 128))(noise_layer)\n",
    "                                       \n",
    "    concatenated_input = Concatenate()([noise_layer, label_layer])\n",
    "    \n",
    "    layer = Conv2DTranspose(filters=128, kernel_size=(4,4), strides=(2,2), padding='same')(concatenated_input)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "    \n",
    "    layer = Conv2DTranspose(filters=128, kernel_size=(4,4), strides=(2,2), padding='same')(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "    \n",
    "    output_layer = Conv2D(filters=1, kernel_size=(7,7), activation=tanh, padding='same')(layer)\n",
    "    \n",
    "    return Model([noise_input, label_input], output_layer)\n",
    "\n",
    "\n",
    "def connect_models(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gen_noise, gen_label = generator.input\n",
    "    \n",
    "    connected_model_output = discriminator([generator.output, gen_label])\n",
    "    \n",
    "    connected_model = Model([gen_noise, gen_label], connected_model_output)\n",
    "    connected_model.compile(loss=BinaryCrossentropy(), optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "    \n",
    "    return connected_model\n",
    " \n",
    "\n",
    "\n",
    "def random_arange(start, stop):\n",
    "    array = np.arange(start, stop)\n",
    "    np.random.shuffle(array)\n",
    "    return array\n",
    "\n",
    "\n",
    "\n",
    "def train_discriminator_on_batch(discriminator, generator, hidden_dim, X_real, labels_real, classes_number):\n",
    "    batch_size_half = X_real.shape[0]\n",
    "    \n",
    "    d_loss_real = discriminator.train_on_batch([X_real, labels_real], np.ones(batch_size_half))\n",
    "\n",
    "    labels_fake = np.random.randint(0, classes_number, size=batch_size_half)\n",
    "    X_fake = generator.predict([np.random.normal(size=(batch_size_half, hidden_dim)), labels_fake])\n",
    "    d_loss_fake = discriminator.train_on_batch([X_fake, labels_fake], np.zeros(batch_size_half))\n",
    "    \n",
    "    return d_loss_real, d_loss_fake\n",
    "\n",
    "\n",
    "\n",
    "def train_generator_on_batch(connected_model, hidden_dim, classes_number, batch_size):\n",
    "    labels_fake = np.random.randint(0, classes_number, size=batch_size)\n",
    "    return connected_model.train_on_batch([np.random.normal(size=(batch_size, hidden_dim)), labels_fake], np.ones(batch_size))\n",
    "\n",
    "\n",
    "\n",
    "def train(generator, discriminator, hidden_dim, X, y, classes_number=10, epochs=100, batch_size=140, epochs_per_save=5, first_epoch_number=0, save_path='.'):\n",
    "    connected_model = connect_models(generator, discriminator)\n",
    "    \n",
    "    batches_number = X.shape[0] // batch_size\n",
    "    batch_size_half = batch_size // 2\n",
    "    \n",
    "    epochs += first_epoch_number\n",
    "    for i in range(first_epoch_number, epochs):\n",
    "        random_indices = random_arange(0, X.shape[0] // 2) if i % 2 == 0 else random_arange(X.shape[0] // 2, X.shape[0])\n",
    "        \n",
    "        d_loss_real, d_loss_fake, g_loss = 0., 0., 0.\n",
    "        average_time = 0\n",
    "        \n",
    "        for j in range(batches_number):\n",
    "            print(f'Epoch {i + 1}/{epochs}, batch {j + 1}/{batches_number}, average_time {round(average_time, 3) or \"?\"}', end='\\r')\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            \n",
    "            samples_index = random_indices[j * batch_size_half:(j + 1) * batch_size_half]\n",
    "            \n",
    "            d_loss_real_batch, d_loss_fake_batch = train_discriminator_on_batch(\n",
    "                discriminator, generator, hidden_dim, \n",
    "                X[samples_index], y[samples_index], classes_number\n",
    "            )\n",
    "            d_loss_real += d_loss_real_batch\n",
    "            d_loss_fake += d_loss_fake_batch\n",
    "            \n",
    "            g_loss += train_generator_on_batch(connected_model, hidden_dim, classes_number, batch_size)\n",
    "            \n",
    "            average_time = (average_time * j + time.perf_counter() - start_time) / (j + 1)\n",
    "                \n",
    "        print(f'Epoch {i + 1}/{epochs}, time {average_time * batches_number:.3f}, d_real={d_loss_real:.3f}, d_fake={d_loss_fake:.3f}, g={g_loss:.3f}')\n",
    "        if (i + 1) % epochs_per_save == 0:\n",
    "            generator.save(os.path.join(save_path, f'generator_{i + 1}.h5'))\n",
    "            discriminator.save(os.path.join(save_path, f'discriminator_{i + 1}.h5'))\n",
    "            print('Saved generator and discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64043242",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 100\n",
    "\n",
    "discriminator = create_discriminator()\n",
    "generator = create_generator(hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(generator, discriminator, hidden_dim, X, y, epochs=200, save_path='models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653ccbc-c5a8-4b6b-a14f-3789e12835ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m70",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m70"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
